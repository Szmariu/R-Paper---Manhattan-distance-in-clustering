---
title: "In Defense of Manhattan Distance - On the Performance Impact of Distance Metrics in Clustering Analysis"
author: "Michał Szałański"
date: "6 lutego 2019"
output: html_document
---


# Introduction
## Goal of the paper
The idea for this paper came to me when I first learned about the different distance metrics. During that class it was said, that there isn't a well known use case of the manhattan distance metric. That got me curious, so I search the internet for papers and stack overflow posts that could tell more on this topic. One of the few ideas that I was able to find, was that it may lead to increased performance because it uses modulus, and not exponentiation. In this paper, I would like to test this hypothesis, comparing manhattan distance to euclidean and a few other distance metrics, to see if it makes a difference in both synthetic benchmarks and real-worlds applications. 

## Theoretical introduction
### Euclidean distance

The **euclidean distance** is "straight line" distance, calculated as a square root of a sum of squared differences between each axis. It is by far the most popular metric, and many libraries use it as a default option.

```{r, echo=FALSE, out.width = "500px", fig.align = "center"}
knitr::include_graphics("assets/euclidean.svg")
```

<center>*The equation for the euclidean distance. Source: Wikipedia*</center>

### Manhattan distance
The **manhatan distance** is calculated as a sum of unsigned lengths on each axis. Because it uses absolute values instead of exponentiation and rooting, it is said to be more robust to outliers. Additionally, because modulus is much faster to compute than exponentiation, it may give a performance increase in certain applications.
 
```{r, echo=FALSE, out.width = "300px", fig.align = "center"}
knitr::include_graphics("assets/Manhattan_distance.svg")
```
 
<center>*Red, blue and yellow lines represent same distances in the manhattan geometry, and the green line represents the euclidean distance. Source: Wikipedia*</center>

### Canberra distance
Canberra distance is a weighted version of the manhattan distance. It is much more robust to outliers than other metrics, but is very sensible to values around 0. This feature makes it a good tool to detect some kinds of outliers, that's why it is used mostly in spam detection software. Since it's more complex than manhattan distance, it may be slower in some applications.

```{r, echo=FALSE, out.width = "200px", fig.align = "center"}
knitr::include_graphics("assets/canberra.svg")
```

<center>*The equation for the canberra distance. Source: Wikipedia*</center>

# Data set overview

## Libraries
```{r, message = FALSE}
library(plyr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(gridExtra)
library(cluster)
library(factoextra)
library(flexclust)
library(fpc)
library(clustertend)
library(ClusterR)
library(NbClust)
library(microbenchmark)
library(multcomp)
library(knitr)

options(scipen=999) #avoiding e10 notation
```

## Importing the data set
This data was cleaned and prepared as a part of an another project. It is a scrapping of Google Play Store apps. It can be found on [Kaggle](https://www.kaggle.com/lava18/google-play-store-apps).

```{r}
# Importing main data table, NaN are ommited (about 10% of all data)
mainTable <- na.omit(read.csv("data/clean_googleplaystore.csv",header=TRUE))
```

## Data overview
After loading the data set, we can look at it's shape.
```{r, comment=NA}
dim(mainTable)
```

The data consists of 7 671 observations and 11 variables. Overall, this gives us 84 381 data points.

Next, we can see what are the variables in this data set.   
```{r, comment=NA}
str(mainTable)
```

There are 11 columns, 5 of them are numeric, the rest are factors. We'll focus on the numeric variables, since they are the easiest to cluster.

We can take a look at a summary of these numeric variables.  
```{r}
kable(summary(mainTable[,c(2,3,4,7,11)]))
```

The ratings count, app size and price variables have some serious outliers, but this shouldn't affect the results of performance analysis in a significant way. 

# Empirical study
## Data preparation

First, we want to extract the two variables that we'll be working on - average rating, and number of reviews.

```{r, comment=NA}
clusteringData <- mainTable[,c(2,3)]
dim(clusteringData)
```

Then, we run NbClust to find a optimal number of clusters. Since our main goal is to evaluate performance, the number of clusters will stay the same for different clustering methods, and distance metrics.

```{r, optimal number of clusters, cache = TRUE, comment=NA}
c3<-NbClust(clusteringData , distance="euclidean", min.nc=2, max.nc=8, method="complete", index="ch")
c3$Best.nc
```

It turns out that for this data, the optimal number of clusters is 6. 

## First performance comparison - distance metrics
### Sample distance matrix
Since we want to compare performance of different metrics, the most significant differences should occur when calculating the distance matrices. This will allow to ignore other factors, such as the implementation of different metrics in the clustering algorithms (we'll get back to that later).

```{r, sample distance matrix, cache = TRUE, comment=NA}
head(dist(clusteringData, method = "euclidean"))
```

### Performance evaluation
To evaluate performance, I'll be using the microbenchmark function from microbenchmark package to calculate distance matrices, each one 300 times. The result is a table containing times for each run. Microbenchmark is a great tool for this application, because it's optimized to exclude any overhead, such as time spent checking system time. It is said to be able to record differences on the scale of nanoseconds. 

```{r, Distance Matrix performance, cache = TRUE}
timeDistanceMatrix <- microbenchmark(
  dist(clusteringData, method = "euclidean"),
  dist(clusteringData, method = "manhattan"),
  dist(clusteringData, method = "canberra"),
  dist(clusteringData, method = "minkowski"),
  times=300)
```

### Results
One we run the tests, we can display the results in a table format and as a graph.

```{r, results='hide'}
timeDistanceMatrix$expr <- revalue(timeDistanceMatrix$expr, c('dist(clusteringData, method = "euclidean")' = "Euclidean",
                                                              'dist(clusteringData, method = "manhattan")' = "Manhattan",
                                                              'dist(clusteringData, method = "canberra")' = 'Canberra',
                                                              'dist(clusteringData, method = "minkowski")' = 'Minkowski'))
timeDistanceMatrixResults <- print(timeDistanceMatrix, unit = "s", order = 'median', signif = 3)
```

```{r, fig.align='center', message = FALSE}
kable(timeDistanceMatrixResults)
```

The results are promising - the manhattan distance is noticeably faster than all other distance metrics. When comparing medians, the euclidean distance is **`r sprintf("%1.2f%%", (timeDistanceMatrixResults[2,5] - timeDistanceMatrixResults[1,5]) / timeDistanceMatrixResults[1,5]*100) `** slower, the Canberra is **`r sprintf("%1.2f%%", (timeDistanceMatrixResults[3,5] - timeDistanceMatrixResults[1,5]) / timeDistanceMatrixResults[1,5]*100) `** slower. By far the slowest is the Minkowski distance metric - it's **`r sprintf("%1.2f%%", (timeDistanceMatrixResults[4,5] - timeDistanceMatrixResults[1,5]) / timeDistanceMatrixResults[1,5]*100) `** slower than Manhattan distance. Results of this "synthetic" benchmark show us that, in a best case scenario, we can expect clustering algorithms to run about 10% faster when using manhattan distance compared to euclidean. 

```{r, fig.align='center', message = FALSE}
ggplot2::autoplot(timeDistanceMatrix)
```

We can see the differences visually. There are strange "tails" on each test - these are outliers that will be analysed later. I've compared medians in the interpretation above to filter out the effects of this strange behavior.

### T-test

```{r, comment=NA}
x <- timeDistanceMatrix %>% filter(timeDistanceMatrix$expr == 'Euclidean') %>% transmute(time = time/1000000000)
y <- timeDistanceMatrix %>% filter(timeDistanceMatrix$expr == 'Manhattan') %>% transmute(time = time/1000000000)
t.test(x$time,y$time)
```

According to the T-test, we can reject the null hypothesis, which says that the means of this two tests are equal. The resulting difference is statistically significant, with a very low p-value.

### Investigating the outliers - how are they distributed?

From the graph above, we can see that there are some outliers in each method. Maybe these are the first runs, after which the dist() function does some caching or other optimization? To test this, we can look at the run times in order. 
 
```{r, fig.align='center'}
timeDistanceMatrix %>% 
  filter(timeDistanceMatrix$expr == 'Euclidean') %>% 
  droplevels.data.frame() %>%
  ggplot(aes(y=time, x=c(1:300))) + geom_point()+ xlab('Time') + ylab('Run time')
```

It doesn't seem like that's the case - the outliers are scattered quite evenly across the time. I wasn't able to find any information on this - it might be caused by a bug in the dist() function, or an background app taking the focus of the processor for a couple of milliseconds (e.g. update software, or cloud syncing apps).

## Second performance comparison - CClust
### Sample clustering in CClust
After a synthetic benchmark that was the distance matrices, we can move on to a first real-world example. The CClust function from flexclust package can perform the kmeans clustering. As of today, it supports two distance metrics - euclidean and manhattan. To show how it looks, we can run a sample clustering on our data. Since this package does not support the ggplot2, the resulting graph leaves much to be desired. 
```{r, fig.align='center', comment=NA,  message = FALSE}
sampleCClust <- cclust(clusteringData, k = 6, dist = "euclidean", simple = FALSE, save.data=TRUE)
plot(sampleCClust)
```

### Performance evaluation
Now, we can evaluate the performance. Both tests were run 200 times, with the number of clusters set to 6 (as was suggested by the NbClust). The results are then saved for further analysis.

```{r, CCLust performance, cache = TRUE, message=FALSE}
timeCClust <- microbenchmark(
  cclust(clusteringData, k = 6, dist = "euclidean", simple = FALSE, save.data=TRUE),
  cclust(clusteringData, k = 6, dist = "manhattan", simple = FALSE, save.data=TRUE),
  times=200)
```

### Results

```{r, results='hide'}
timeCClust$expr <- revalue(timeCClust$expr, c('cclust(clusteringData, k = 6, dist = \"euclidean\", simple = FALSE,      save.data = TRUE)' = "Euclidean",
                                          'cclust(clusteringData, k = 6, dist = \"manhattan\", simple = FALSE,      save.data = TRUE)' = "Manhattan"))
timeCClustResults <- print(timeCClust, unit = "s", order = 'median', signif = 3)
```

```{r, fig.align='center', message = FALSE}
kable(timeCClustResults)
```

The results are somewhat surprising. Contrary to the experiment on the distance matrices, it seems that manhattan distance is slower in CClust algorithm, by **`r sprintf("%1.2f%%", (timeCClustResults[2,5] - timeCClustResults[1,5]) / timeCClustResults[1,5]*100) `**.  This maybe caused by the difference in implementation - when using euclidean, the cluster means are the centroids, and in manhattan, the column-wise cluster medians are used as centroids.  

```{r, fig.align='center', message = FALSE}
ggplot2::autoplot(timeCClust)
```

Strange outliers can be observed, similar to these in distance matrices. After a short analysis of the CClust source code, I was able to find that it indeed uses the dist() function, which may have caused the outliers.  

### T-test

```{r, comment=NA}
x <- timeCClust %>% filter(timeCClust$expr == 'Euclidean') %>% transmute(time = time/1000000000)
y <- timeCClust %>% filter(timeCClust$expr == 'Manhattan') %>% transmute(time = time/1000000000)
t.test(x$time,y$time)
```

According to the T-test, we can say that the resulting difference is statistically significant, with a low p-value.

## Third performance comparison - Clara

### Sample clustering in Clara
Below, we can see a sample clustering done in clara. Number of samples, and the sample size was increased from defaults, as is suggested in the documentation.   
The silhouette plot indicates that the resulting clustering is quite good, with a average silhouette width of 0.82. 
```{r, fig.align='center', comment=NA}
sampleClara<-clara(clusteringData, 6, metric="euclidean", stand=FALSE, samples=50,
                   sampsize=200, trace=0, medoids.x=TRUE,
                   rngR=TRUE, pamLike=FALSE, correct.d=TRUE) #cluster::
fviz_cluster(sampleClara, ellipse.type = "t", geom = "point", pointsize = 1 )
fviz_silhouette(sampleClara)
```

### Performance evaluation

Similarly to CClust, the Clara supports two distance metrics - euclidean and manhattan, the former being the default. Since Clara it is noticeably faster than CClust, this test was run 600 times for each metric. Overall it took around 10 minutes to complete. 

```{r, Clara performance, cache = TRUE}
timeClara <- microbenchmark(
  clara(clusteringData, 6, metric="euclidean", stand=FALSE, samples=50,
        sampsize=200, trace=0, medoids.x=TRUE,
        rngR=TRUE, pamLike=FALSE, correct.d=TRUE),
  clara(clusteringData, 6, metric="manhattan", stand=FALSE, samples=50,
        sampsize=200, trace=0, medoids.x=TRUE,
        rngR=TRUE, pamLike=FALSE, correct.d=TRUE),
  times=600)
```

### Results

```{r, results='hide'}
timeClara$expr <- revalue(timeClara$expr, c('clara(clusteringData, 6, metric = "euclidean", stand = FALSE,      samples = 50, sampsize = 200, trace = 0, medoids.x = TRUE,      rngR = TRUE, pamLike = FALSE, correct.d = TRUE)' = "Euclidean",
                                          'clara(clusteringData, 6, metric = "manhattan", stand = FALSE,      samples = 50, sampsize = 200, trace = 0, medoids.x = TRUE,      rngR = TRUE, pamLike = FALSE, correct.d = TRUE)' = "Manhattan"))
timeClaraResults <- print(timeClara, unit = "s", order = 'median', signif = 3)
```


```{r, fig.align='center', message = FALSE}
kable(timeClaraResults)
```

This time the results are more similar to those from the synthetic benchmark. When using the euclidean distance instead of manhattan in the clara algorithm, we can expect it to be around **`r sprintf("%1.2f%%", (timeClaraResults[2,5] - timeClaraResults[1,5]) / timeClaraResults[1,5]*100) `** slower. 


```{r, fig.align='center', message = FALSE}
ggplot2::autoplot(timeClara)
```

This time, the outliers are not present. This would suggest that the outliers are a problem with the dist() function, and not the microbenchmarking process, or some other outside influence. 

### T-test

```{r, comment=NA}
x <- timeClara %>% filter(timeClara$expr == 'Euclidean') %>% transmute(time = time/1000000000)
y <- timeClara %>% filter(timeClara$expr == 'Manhattan') %>% transmute(time = time/1000000000)
t.test(x$time,y$time)
```

Like in previous experiments, the the resulting difference is statistically significant. 

# Conclusions

The conclusions of this experiments are not straightforward. The synthetic benchmark - distance matrices, showed that using a manhattan distance metric can lead to a performance increase of around **10%**. This positive effect was also present when using the Clara algorithm, although to a smaller extent (around **3.5%**). On the contrary, in CClust the use of manhattan distance led to a slowdown of about **7.50%**.   
    
Generally speaking, much bigger performance differences can be achieved by using a different algorithm or implementation (e.g. one using precompiled C code). In the tests above, the difference between CClust and Clara are quite big - **1.03** seconds for CClust, and **0,28** seconds for Clara. But, if the most efficient algorithm and implementation was already chosen, the results from this paper may suggest that testing the performance of different distance metrics may be beneficial. Especially if the algorithm is expected to run more than once. 

# Bibliography

- *A comparative study of K Means Algorithm by Different Distance Measures*, Kahkashan Kouser
- *Comparison of Euclidean Distance Function and Manhattan Distance Function Using K-Mediods*, Md. Mohibullah, Md. Zakir Hossain, Mahmudul Hasan
- *Distance metric learning, with application to clustering with side-information*, Eric P. Xing, Andrew Y. Ng, Michael I. Jordan and Stuart Russell
- *Effect of Different Distance Measures on the Performance of K-Means Algorithm: An Experimental Study in Matlab*, Dibya Jyoti Bora, Dr. Anil Kumar Gupta
- *Study of Euclidean and Manhattan Distance Metrics using Simple K-Means Clustering*, Deepak Sinwar, Rahul Kaushik  




