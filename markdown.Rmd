---
title: "In Defence of Manhattan Distance - On the Performance Impact of Distance Measures in Clustering Analisys"
author: "Michał Szałański"
date: "6 lutego 2019"
output: html_document
---


# Introduction
## Goal of the paper
The idea for this paper came to me when I first learned about the different distance measures. During that class it was said, that no one knows what is the usecase of the manhattan distance metric. That got me curious, so I search the internet for papers and stack overflow posts that could tell more on this topic. One of the few ideads that I was able to find, was that it may lead to increased performance because it uses modulus, and not exponanciation. In this paper, I would like to test this hypothesis, comparing manhattan distance to euclidian and a few other distance metrics, to see if it makes a difference in both syntetic benchmarks and real-worlds examples. 

## Theoretical background
### Euclidian distance

The **euclidian distance** is "straght line" distance, calcualted as a square root of a sum of squared differences beetwen each axis.   

```{r, echo=FALSE, out.width = "500px", fig.align = "center"}
knitr::include_graphics("assets/euclidian.svg")
```

<center>*The equation for the euclidian distance. Source: Wikipedia*</center>

### Manhattan distance
The **manhatan distance** is calculated as a sum of unsigned lengths on each axis. Because it uses modulus instead of exponenciation and rooting, it is said to be more roboust to outliers. Aditionally, because modulus is much faster to compute than exponenciation and square rooting, it may give a performance increase in certain applications.
 
```{r, echo=FALSE, out.width = "300px", fig.align = "center"}
knitr::include_graphics("assets/Manhattan_distance.svg")
```
 
<center>*Red, blue and yellow lines represent same dsitances in the manhattan geometry, and the green line represents the euclidian distance. Source: Wikipedia*</center>

### Canberra distance
Canberra distance is a weighted version of the manhattan distance. It is much more robust to outliers than other metrics, but is very sensible to values around 0. This feature makes it a good tool to detect some kinds of outliers. It is used mostly in spam detection software. Since it's more complex than manhattan distance, it may be slower in some applications.

```{r, echo=FALSE, out.width = "200px", fig.align = "center"}
knitr::include_graphics("assets/canberra.svg")
```

<center>*The equation for the canberra distance. Source: Wikipedia*</center>

# Dataset overview

## Libraries
```{r, message = FALSE}
library(plyr)
library(dplyr)
library(tidyr)
library(ggplot2)
#library(gridExtra)
#library(cluster)
library(factoextra)
library(flexclust)
library(fpc)
library(clustertend)
library(ClusterR)
library(NbClust)
library(microbenchmark)
library(multcomp)
library(knitr)

options(scipen=999) #avoiding e10 notation
```

## Importing the dataset
This data was cleaned and prepared as a part of an another project. The source is [Kaggle](https://www.kaggle.com/lava18/google-play-store-apps), and it's a scrapping of a sample of Google Play Store apps.

```{r}
# Importing main data table, NaN are ommited (about 10% of all data)
mainTable <- na.omit(read.csv("data/clean_googleplaystore.csv",header=TRUE))

# Import the ratings, NaN are ommited (about 30% of all data)
ratingsTable <- na.omit(read.csv("data/clean_googleplaystore_user_reviews.csv",header=TRUE))
```

## Data overview
After loading the dataset, we can look at it's shape.
```{r, comment=NA}
dim(mainTable)
```

The data consists of 7 671 observations and 11 variables. Overall, this gives us 84 381 data points.

Next, we can see what are the variables in this dataset.   
```{r, comment=NA}
str(mainTable)
```

There are 11 columns, 5 of them are continous, the rest are factors. We'll focus on the continous variables, since they are the easies to cluster.

We can take a look at a summary of these continous variables.  
```{r}
kable(summary(mainTable[,c(2,3,4,7,11)]))
```

The ratings count, app size and price variables have some serious outliers, but this shouldn't affect the results of performance analisys in any significant way. 

# Empyrical study
## Data preparation

First, we want to extract the two variables that we'll be working on - average rating, and number of reviews.

```{r, comment=NA}
clusteringData <- mainTable[,c(2,3)]
dim(clusteringData)
```

Then, we run NbClust to find a optimal number of clusters. Since our main goal is to evaluate performance, the number of clusters will stay the same for different clustering methods, and distance metrics.

```{r, optimal number of clusters, cache = TRUE, comment=NA}
c3<-NbClust(clusteringData , distance="euclidean", min.nc=2, max.nc=8, method="complete", index="ch")
c3$Best.nc
```

It turns out, that for this data, the optimal number of clusters is 6. 

## First perfomance comparison - distance metrics
### Sample distance matrix
Since we want to compare performance of different metrics, the most significant diferences should occour when calculating the distance matrixes. This will allow to ignore other factors, such as the implementation of different metrics in the clustering algorithms (we'll get back to that later).

```{r, sample distance matrix, cache = TRUE, comment=NA}
head(dist(clusteringData, method = "euclidean"))
```

### Performance evaluation
Here, I'm using the microbenrchmark function from microbenchmark package to calculate distance matrixes, each one 300 times. The result is a table containing the times for each run. Microbenchmark is a great tool for this application, because it's optimised to exclude any overhead, such as checking system time. Authors claim that it can record differences on the scale of nanoseconds. 

```{r, Distance Matrix performance, cache = TRUE}
timeDistanceMatrix <- microbenchmark(
  dist(clusteringData, method = "euclidean"),
  dist(clusteringData, method = "manhattan"),
  dist(clusteringData, method = "canberra"),
  dist(clusteringData, method = "minkowski"),
  times=300)
```

### Results
One we run the tests, we can display the results in a table format, and as a graph.

```{r, results='hide'}
timeDistanceMatrix$expr <- revalue(timeDistanceMatrix$expr, c('dist(clusteringData, method = "euclidean")' = "Euclidian",
                                          'dist(clusteringData, method = "manhattan")' = "Manhattan",
                                          'dist(clusteringData, method = "canberra")' = 'Canberra',
                                          'dist(clusteringData, method = "minkowski")' = 'Minkowski'))
timeDistanceMatrixResults <- print(timeDistanceMatrix, unit = "s", order = 'median', signif = 3)
```

```{r, fig.align='center', message = FALSE}
kable(timeDistanceMatrixResults)
```

The results are promising - the manhattan distance is noticabely faster than all other distance measures. When comparing medians, the euclidian distance is **`r sprintf("%1.2f%%", (timeDistanceMatrixResults[2,5] - timeDistanceMatrixResults[1,5]) / timeDistanceMatrixResults[1,5]*100) `** slower, the canrerra is **`r sprintf("%1.2f%%", (timeDistanceMatrixResults[3,5] - timeDistanceMatrixResults[1,5]) / timeDistanceMatrixResults[1,5]*100) `** slower. By far the slowest is the Minkowski distance measure - it's **`r sprintf("%1.2f%%", (timeDistanceMatrixResults[4,5] - timeDistanceMatrixResults[1,5]) / timeDistanceMatrixResults[1,5]*100) `** slower than manhatta distance. Results of this "syntetic" benchmark show as, that is a best case scenario, we can expect clustering algorithms to run about 10% faster, using manhattan distance, than euclidian. 

```{r, fig.align='center', message = FALSE}

ggplot2::autoplot(timeDistanceMatrix)
```

We can see the differences visually. There are strange "tails" on each test - these are outliers that will be analised later. During the interpretation above, I've compared medians, to filter out this strange behaviour.

### T-test

```{r, comment=NA}
x <- timeDistanceMatrix %>% filter(timeDistanceMatrix$expr == 'Euclidian') %>% transmute(time = time/1000000000)
y <- timeDistanceMatrix %>% filter(timeDistanceMatrix$expr == 'Manhattan') %>% transmute(time = time/1000000000)
t.test(x$time,y$time)
```

According to the T-test, we can reject the null hypothesis, which says that the means of this two tests are equal. The resulting difference is statistically significant, with a very low p-value.

### Investigating the outliers - how are they distributed?

From the graph above, we can see that there are some outliers in each method. Maybe these are the first runs, after which the dist() function does some caching or other optimisation? To test this, we can look at the run times in order. 
 
```{r, fig.align='center'}
timeDistanceMatrix %>% 
  filter(timeDistanceMatrix$expr == 'Euclidian') %>% 
  droplevels.data.frame() %>%
  ggplot(aes(y=time, x=c(1:300))) + geom_point()+ xlab('Time') + ylab('Runtime')
```

It doesn't seem like that's the case - the outliers are scattered roughly evenly across the time. I wasn't able to find any information on this - maybe it is caused by a bug in the dist() function, or an backgroud app taking the focus of the processor for a couple of miliseconds (eg. updaters, or cloud syncing apps).

## Second perfomance comparison - CClust
### Sample clustering in CClust


### Performance evaluation
After a syntetic benchmark that was the distance matrixes, we can move on to a first real-world example. The CClust function from flexclust package can perform the kmeans clustering. As of today, it supports two distance measures - euclidian and manhattan. Both tests were run 200 times, with the number of clusters set to 6 (as was estabilished by the NbClust). The results are then saved for futher analisys.

```{r, CCLust performance, cache = TRUE, message=FALSE}
timeCClust <- microbenchmark(
  cclust(clusteringData, k = 6, dist = "euclidean", simple = FALSE, save.data=TRUE),
  cclust(clusteringData, k = 6, dist = "manhattan", simple = FALSE, save.data=TRUE),
  times=200)
```

### Results

```{r, results='hide'}
timeCClust$expr <- revalue(timeCClust$expr, c('cclust(clusteringData, k = 6, dist = \"euclidean\", simple = FALSE,      save.data = TRUE)' = "Euclidian",
                                          'cclust(clusteringData, k = 6, dist = \"manhattan\", simple = FALSE,      save.data = TRUE)' = "Manhattan"))
timeCClustResults <- print(timeCClust, unit = "s", order = 'median', signif = 3)
```

```{r, fig.align='center', message = FALSE}
kable(timeCClustResults)
```

The results are somewhat suprising. Contrary to the experiment on the distance matrixes, it seems that manhattan distance is slower in CClust algorithm, by **`r sprintf("%1.2f%%", (timeCClustResults[2,5] - timeCClustResults[1,5]) / timeCClustResults[1,5]*100) `**.  This maybe caused by the difference in implementation - in euclidian setting, the cluster means are the centroids, and in the manhattan, the column-wise cluster medians are used as centroids.  

```{r, fig.align='center', message = FALSE}
ggplot2::autoplot(timeCClust)
```

Strange outliers can be observed, simmilar to these in distance matrixes. Maybe CClust uses the dist() function a as part of it's algorithm?

### T-test

```{r, comment=NA}
x <- timeCClust %>% filter(timeCClust$expr == 'Euclidian') %>% transmute(time = time/1000000000)
y <- timeCClust %>% filter(timeCClust$expr == 'Manhattan') %>% transmute(time = time/1000000000)
t.test(x$time,y$time)
```

According to the T-test, we can say that the resulting difference is statistically significant, with a very low p-value.

## Third perfomance comparison - Clara

### Sample clustering in Clara
```{r, fig.align='center', comment=NA}
sampleClara<-clara(clusteringData, 6, metric="euclidean", stand=FALSE, samples=50,
                   sampsize=200, trace=0, medoids.x=TRUE,
                   rngR=TRUE, pamLike=FALSE, correct.d=TRUE) #cluster::
fviz_cluster(sampleClara, ellipse.type = "t", geom = "point", pointsize = 1 )
fviz_silhouette(sampleClara)
```

### Performance evaluation

```{r, Clara performance, cache = TRUE}
timeClara <- microbenchmark(
  clara(clusteringData, 6, metric="euclidean", stand=FALSE, samples=50,
        sampsize=200, trace=0, medoids.x=TRUE,
        rngR=TRUE, pamLike=FALSE, correct.d=TRUE),
  clara(clusteringData, 6, metric="manhattan", stand=FALSE, samples=50,
        sampsize=200, trace=0, medoids.x=TRUE,
        rngR=TRUE, pamLike=FALSE, correct.d=TRUE),
  times=600)
```

### Results

```{r, results='hide'}
timeClara$expr <- revalue(timeClara$expr, c('clara(clusteringData, 6, metric = "euclidean", stand = FALSE,      samples = 50, sampsize = 200, trace = 0, medoids.x = TRUE,      rngR = TRUE, pamLike = FALSE, correct.d = TRUE)' = "Euclidian",
                                          'clara(clusteringData, 6, metric = "manhattan", stand = FALSE,      samples = 50, sampsize = 200, trace = 0, medoids.x = TRUE,      rngR = TRUE, pamLike = FALSE, correct.d = TRUE)' = "Manhattan"))
timeClaraResults <- print(timeClara, unit = "s", order = 'median', signif = 3)
```

```{r, fig.align='center', message = FALSE}
kable(timeClaraResults)
ggplot2::autoplot(timeClara)
```


### T-test

```{r, comment=NA}
x <- timeClara %>% filter(timeClara$expr == 'Euclidian') %>% transmute(time = time/1000000000)
y <- timeClara %>% filter(timeClara$expr == 'Manhattan') %>% transmute(time = time/1000000000)
t.test(x$time,y$time)
```


## What about (ten algo który nie działa)


# Conclusions

# References
